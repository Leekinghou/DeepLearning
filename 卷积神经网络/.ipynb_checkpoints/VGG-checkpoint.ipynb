{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5bac3a5",
   "metadata": {},
   "source": [
    "# VGG块网络\n",
    "            ——可定制的AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24787796",
   "metadata": {},
   "source": [
    "![](https://gitee.com/leekinghou/image/raw/master/img/20211125104107.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bd09d5",
   "metadata": {},
   "source": [
    "## VGG块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f5f982",
   "metadata": {},
   "source": [
    "经典卷积神经网络的基本组成部分是下面的这个序列： \n",
    "1. 带填充以保持分辨率的卷积层； \n",
    "1. 非线性激活函数，如ReLU； \n",
    "1. 汇聚层，如最大汇聚层。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846669da",
   "metadata": {},
   "source": [
    "而一个 VGG 块与之类似，由一系列卷积层组成，后面再加上用于空间下采样的最大汇聚层。在最初的 VGG 论文 [Simonyan & Zisserman, 2014] 中，作者使用了带有 $3\\times3$ 卷积核、填充为 1（保持高度和宽度）的卷积层，和带有 $2 \\times 2$ 池化窗口、步幅为 2（每个块后的分辨率减半）的最大汇聚层。在下面的代码中，我们定义了一个名为 vgg_block 的函数来实现一个 VGG 块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2d05ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d4a062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_block(num_convs, in_channels, out_channels):\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        # padding = 1表示输入输出的高宽一样\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        in_channels = out_channels\n",
    "    # stride = 2 步长为2\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e615a483",
   "metadata": {},
   "source": [
    "## VGG网络\n",
    "与 AlexNet、LeNet 一样，VGG 网络可以分为两部分：第一部分主要由卷积层和汇聚层组成，第二部分由全连接层组成。如 图7.2.1 中所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580d4cd0",
   "metadata": {},
   "source": [
    "![](https://gitee.com/leekinghou/image/raw/master/img/1637808333719.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a957069d",
   "metadata": {},
   "source": [
    "![](https://gitee.com/leekinghou/image/raw/master/img/20211125173938.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cd7a32",
   "metadata": {},
   "source": [
    "VGG神经网络连续连接 图7.2.1 的几个 VGG 块（在 vgg_block 函数中定义）。其中有超参数变量**conv_arch** 。该变量指定了每个VGG块里卷积层**个数和输出通道数**。全连接模块则与AlexNet中的相同。\n",
    "\n",
    "原始 VGG 网络有 **5 个卷积块**，其中**前两个块**各有**一个**卷积层，后三个块各包含两个卷积层。 第一个模块有 64 个输出通道，每个后续模块将输出通道数量翻倍，直到该数字达到 512。由于该网络使用 8 个卷积层和 3 个全连接层，因此它通常被称为 VGG-11。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01681608",
   "metadata": {},
   "source": [
    "[卷积基本知识](https://zhuanlan.zhihu.com/p/77471866)\n",
    "\n",
    "[卷积核与通道](https://zhuanlan.zhihu.com/p/251068800)\n",
    "\n",
    "总之：  \n",
    "1. 输入通道个数 = 卷积核通道个数  \n",
    "2. 卷积核个数 = 输出通道个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b7bf0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43efa33",
   "metadata": {},
   "source": [
    "下面的代码实现了 VGG-11。可以通过在 conv_arch 上执行 for 循环来简单实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "006cf569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg(conv_arch):\n",
    "    conv_blks = []\n",
    "    in_channels = 1\n",
    "    # 卷积层部分\n",
    "    for (num_convs, out_channels) in conv_arch:\n",
    "        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))\n",
    "        in_channels = out_channels\n",
    "\n",
    "    return nn.Sequential(\n",
    "        *conv_blks, nn.Flatten(),\n",
    "        # 全连接层部分\n",
    "        nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 10))\n",
    "\n",
    "net = vgg(conv_arch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390e8452",
   "metadata": {},
   "source": [
    "接下来，我们将构建一个高度和宽度为 224 的单通道数据样本，以观察每个层输出的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d175349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baikal/miniforge3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 112, 112])\n",
      "Sequential output shape:\t torch.Size([1, 128, 56, 56])\n",
      "Sequential output shape:\t torch.Size([1, 256, 28, 28])\n",
      "Sequential output shape:\t torch.Size([1, 512, 14, 14])\n",
      "Sequential output shape:\t torch.Size([1, 512, 7, 7])\n",
      "Flatten output shape:\t torch.Size([1, 25088])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(size=(1, 1, 224, 224))\n",
    "for blk in net:\n",
    "    X = blk(X)\n",
    "    print(blk.__class__.__name__,'output shape:\\t',X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33a4579",
   "metadata": {},
   "source": [
    "正如你所看到的，我们在每个块的高度和宽度减半，最终高度和宽度都为7。最后再展平表示，送入全连接层处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4f51d7",
   "metadata": {},
   "source": [
    "## 训练模型\n",
    "由于VGG-11比AlexNet计算量更大，因此我们构建了一个通道数较少的网络，足够用于训练Fashion-MNIST数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5df908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 4\n",
    "small_conv_arch = [(pair[0], pair[1] // ratio) for pair in conv_arch]\n",
    "net = vgg(small_conv_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5a4ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs, batch_size = 0.05, 10, 128\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)\n",
    "d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())\n",
    "# 下图是在服务器上的运行结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd54db2",
   "metadata": {},
   "source": [
    "![](https://gitee.com/leekinghou/image/raw/master/img/20211125111458.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34c04b8",
   "metadata": {},
   "source": [
    "## 小结\n",
    "1. VGG-11 使用可复用的卷积块构造网络。不同的 VGG 模型可通过每个块中卷积层数量和输出通道数量的差异来定义。\n",
    "\n",
    "2. 块的使用导致网络定义的非常简洁。使用块可以有效地设计复杂的网络。\n",
    "\n",
    "2. 在VGG论文中，Simonyan和Ziserman尝试了各种架构。特别是他们发现深层且窄的卷积（即$3 \\times 3$）比较浅层且宽的卷积更有效。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca08f575",
   "metadata": {},
   "source": [
    "![](https://gitee.com/leekinghou/image/raw/master/img/20211125111957.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8be6204",
   "metadata": {},
   "source": [
    "表格参数详解：\n",
    "\n",
    "GPU：本机中的GPU编号（有多块显卡的时候，从0开始编号）图上GPU的编号是：0\n",
    "\n",
    "Fan：风扇转速（0%-100%），N/A表示没有风扇\n",
    "\n",
    "Name：GPU类型，图上GPU的类型是：NVIDIA\n",
    "\n",
    "Temp：GPU的温度（GPU温度过高会导致GPU的频率下降）\n",
    "\n",
    "Perf：GPU的性能状态，从P0（最大性能）到P12（最小性能），图上是：P2\n",
    "\n",
    "Persistence-M：持续模式的状态，持续模式虽然耗能大，但是在新的GPU应用启动时花费的时间更少，图上显示的是：off\n",
    "\n",
    "Pwr：Usager/Cap：能耗表示，Usage：用了多少，Cap总共多少\n",
    "\n",
    "Bus-Id：GPU总线相关显示，domain：bus：device.function\n",
    "\n",
    "Disp.A：Display Active ，表示GPU的显示是否初始化\n",
    "\n",
    "Memory-Usage：显存使用率\n",
    "\n",
    "Volatile GPU-Util：GPU使用率\n",
    "\n",
    "Uncorr. ECC：关于ECC的东西，是否开启错误检查和纠正技术，0/disabled,1/enabled\n",
    "\n",
    "Compute M：计算模式，0/DEFAULT,1/EXCLUSIVE_PROCESS,2/PROHIBITED\n",
    "\n",
    "Processes：显示每个进程占用的显存使用率、进程号、占用的哪个GPU\n",
    "\n",
    "显存占用和GPU占用是两个不一样的东西，显卡是由GPU和显存等组成的，显存和GPU的关系有点类似于内存和CPU的关系。跑caffe代码的时候显存占得少，GPU占得多，跑TensorFlow代码的时候，显存占得多，GPU占得少。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128eb6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
